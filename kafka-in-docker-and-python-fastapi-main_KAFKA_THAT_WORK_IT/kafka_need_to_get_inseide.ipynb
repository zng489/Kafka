{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from time import sleep\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#channel\n",
    "topic='app'\n",
    "\n",
    "# producer\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'],value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "#consumer\n",
    "consumer = KafkaConsumer(topic, bootstrap_servers=['localhost:9092'],auto_offset_reset='earliest',value_deserializer=lambda x:json.loads(x.decode('utf-8')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KafkaTimeoutError",
     "evalue": "KafkaTimeoutError: Failed to update metadata after 60.0 secs.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKafkaTimeoutError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mnumber\u001b[39m\u001b[39m'\u001b[39m : e}\n\u001b[1;32m----> 3\u001b[0m     producer\u001b[39m.\u001b[39;49msend(topic, value\u001b[39m=\u001b[39;49mdata)\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(data)\n\u001b[0;32m      5\u001b[0m     sleep(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\producer\\kafka.py:576\u001b[0m, in \u001b[0;36mKafkaProducer.send\u001b[1;34m(self, topic, value, key, headers, partition, timestamp_ms)\u001b[0m\n\u001b[0;32m    574\u001b[0m key_bytes \u001b[39m=\u001b[39m value_bytes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 576\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_on_metadata(topic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mmax_block_ms\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m/\u001b[39;49m \u001b[39m1000.0\u001b[39;49m)\n\u001b[0;32m    578\u001b[0m     key_bytes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serialize(\n\u001b[0;32m    579\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mkey_serializer\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    580\u001b[0m         topic, key)\n\u001b[0;32m    581\u001b[0m     value_bytes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serialize(\n\u001b[0;32m    582\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mvalue_serializer\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    583\u001b[0m         topic, value)\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\producer\\kafka.py:702\u001b[0m, in \u001b[0;36mKafkaProducer._wait_on_metadata\u001b[1;34m(self, topic, max_wait)\u001b[0m\n\u001b[0;32m    700\u001b[0m elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m begin\n\u001b[0;32m    701\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m metadata_event\u001b[39m.\u001b[39mis_set():\n\u001b[1;32m--> 702\u001b[0m     \u001b[39mraise\u001b[39;00m Errors\u001b[39m.\u001b[39mKafkaTimeoutError(\n\u001b[0;32m    703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to update metadata after \u001b[39m\u001b[39m%.1f\u001b[39;00m\u001b[39m secs.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (max_wait,))\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m topic \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39munauthorized_topics:\n\u001b[0;32m    705\u001b[0m     \u001b[39mraise\u001b[39;00m Errors\u001b[39m.\u001b[39mTopicAuthorizationFailedError(topic)\n",
      "\u001b[1;31mKafkaTimeoutError\u001b[0m: KafkaTimeoutError: Failed to update metadata after 60.0 secs."
     ]
    }
   ],
   "source": [
    "for e in range(5):\n",
    "    data = {'number' : e}\n",
    "    producer.send(topic, value=data)\n",
    "    print(data)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consumer\n",
    "consumer = KafkaConsumer(topic, bootstrap_servers=['localhost:9092'],auto_offset_reset='earliest',value_deserializer=lambda x:json.loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in consumer:\n",
    "    message = message.value\n",
    "    print('Message :{}'.format(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\conn.py:1524\u001b[0m, in \u001b[0;36mdns_lookup\u001b[1;34m(host, port, afi)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(is_inet_4_or_6,\n\u001b[1;32m-> 1524\u001b[0m                        socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, afi,\n\u001b[0;32m   1525\u001b[0m                                           socket\u001b[39m.\u001b[39;49mSOCK_STREAM)))\n\u001b[0;32m   1526\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 954\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[0;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkafka\u001b[39;00m \u001b[39mimport\u001b[39;00m KafkaConsumer\n\u001b[0;32m      3\u001b[0m consumer \u001b[39m=\u001b[39m KafkaConsumer(\u001b[39m'\u001b[39m\u001b[39mpageview\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m                          auto_offset_reset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mearliest\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m                          group_id\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpageview-group1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                          bootstrap_servers\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m:9092\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m consumer:\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39m        topic     => \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mtopic\u001b[39m}\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m        partition => \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mpartition\u001b[39m}\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m        offset    => \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39moffset\u001b[39m}\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m        key=\u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mkey\u001b[39m}\u001b[39;00m\u001b[39m value=\u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mvalue\u001b[39m}\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\consumer\\group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[0;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\consumer\\group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[0;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\consumer\\group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[1;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[0;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[0;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\consumer\\group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[0;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[0;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[0;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\consumer\\group.py:675\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll_once\u001b[39m(\u001b[39mself\u001b[39m, timeout_ms, max_records, update_offsets\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    666\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Do one round of polling. In addition to checking for new data, this does\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[39m    any needed heart-beating, auto-commits, and offset updates.\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[39m        dict: Map of topic to list of records (may be empty).\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coordinator\u001b[39m.\u001b[39;49mpoll()\n\u001b[0;32m    677\u001b[0m     \u001b[39m# Fetch positions if we have partitions we're subscribed to that we\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39m# don't know the offset for\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_subscription\u001b[39m.\u001b[39mhas_all_fetch_positions():\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\coordinator\\consumer.py:289\u001b[0m, in \u001b[0;36mConsumerCoordinator.poll\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m             metadata_update \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mcluster\u001b[39m.\u001b[39mrequest_update()\n\u001b[0;32m    287\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(future\u001b[39m=\u001b[39mmetadata_update)\n\u001b[1;32m--> 289\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_active_group()\n\u001b[0;32m    291\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_heartbeat()\n\u001b[0;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_auto_commit_offsets_async()\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\coordinator\\base.py:353\u001b[0m, in \u001b[0;36mBaseCoordinator.ensure_active_group\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_heartbeat_thread()\n\u001b[0;32m    352\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneed_rejoin() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rejoin_incomplete():\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_coordinator_ready()\n\u001b[0;32m    355\u001b[0m     \u001b[39m# call on_join_prepare if needed. We set a flag\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[39m# to make sure that we do not call it a second\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[39m# time if the client is woken up before a pending\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[39m# changes the matched subscription set) can occur\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[39m# while another rebalance is still in progress.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrejoining:\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\coordinator\\base.py:258\u001b[0m, in \u001b[0;36mBaseCoordinator.ensure_coordinator_ready\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    257\u001b[0m future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup_coordinator()\n\u001b[1;32m--> 258\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(future\u001b[39m=\u001b[39;49mfuture)\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m future\u001b[39m.\u001b[39mfailed():\n\u001b[0;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m future\u001b[39m.\u001b[39mretriable():\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\client_async.py:582\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39m# Attempt to complete pending connections\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[39mfor\u001b[39;00m node_id \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connecting):\n\u001b[1;32m--> 582\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_connect(node_id)\n\u001b[0;32m    584\u001b[0m \u001b[39m# Send a metadata request if needed\u001b[39;00m\n\u001b[0;32m    585\u001b[0m metadata_timeout_ms \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_refresh_metadata()\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\client_async.py:392\u001b[0m, in \u001b[0;36mKafkaClient._maybe_connect\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39melif\u001b[39;00m conn\u001b[39m.\u001b[39mconnected():\n\u001b[0;32m    390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 392\u001b[0m conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m    393\u001b[0m \u001b[39mreturn\u001b[39;00m conn\u001b[39m.\u001b[39mconnected()\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\conn.py:363\u001b[0m, in \u001b[0;36mBrokerConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m ConnectionStates\u001b[39m.\u001b[39mDISCONNECTED \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblacked_out():\n\u001b[0;32m    362\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 363\u001b[0m     next_lookup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_afi_sockaddr()\n\u001b[0;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m next_lookup:\n\u001b[0;32m    365\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose(Errors\u001b[39m.\u001b[39mKafkaConnectionError(\u001b[39m'\u001b[39m\u001b[39mDNS failure\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\conn.py:322\u001b[0m, in \u001b[0;36mBrokerConnection._next_afi_sockaddr\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_afi_sockaddr\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gai:\n\u001b[1;32m--> 322\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_lookup():\n\u001b[0;32m    323\u001b[0m             \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     afi, _, __, ___, sockaddr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gai\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\conn.py:313\u001b[0m, in \u001b[0;36mBrokerConnection._dns_lookup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_dns_lookup\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gai \u001b[39m=\u001b[39m dns_lookup(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhost, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mafi)\n\u001b[0;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gai:\n\u001b[0;32m    315\u001b[0m         log\u001b[39m.\u001b[39merror(\u001b[39m'\u001b[39m\u001b[39mDNS lookup failed for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    316\u001b[0m                   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafi)\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\conn.py:1524\u001b[0m, in \u001b[0;36mdns_lookup\u001b[1;34m(host, port, afi)\u001b[0m\n\u001b[0;32m   1517\u001b[0m \u001b[39m# XXX: all DNS functions in Python are blocking. If we really\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m \u001b[39m# want to be non-blocking here, we need to use a 3rd-party\u001b[39;00m\n\u001b[0;32m   1519\u001b[0m \u001b[39m# library like python-adns, or move resolution onto its\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m \u001b[39m# own thread. This will be subject to the default libc\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[39m# name resolution timeout (5s on most Linux boxes)\u001b[39;00m\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(is_inet_4_or_6,\n\u001b[1;32m-> 1524\u001b[0m                        socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, afi,\n\u001b[0;32m   1525\u001b[0m                                           socket\u001b[39m.\u001b[39;49mSOCK_STREAM)))\n\u001b[0;32m   1526\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m   1527\u001b[0m     log\u001b[39m.\u001b[39mwarning(\u001b[39m'\u001b[39m\u001b[39mDNS lookup failed for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1528\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m exception was \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Is your\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1529\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m advertised.listeners (called\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1530\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m advertised.host.name before Kafka 9)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1531\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m correct and resolvable?\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1532\u001b[0m                 host, port, ex)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer('pageview',\n",
    "                         auto_offset_reset='earliest',\n",
    "                         group_id='pageview-group1',\n",
    "                         bootstrap_servers=[':9092'])\n",
    "for message in consumer:\n",
    "    print(f\"\"\"\n",
    "        topic     => {message.topic}\n",
    "        partition => {message.partition}\n",
    "        offset    => {message.offset}\n",
    "        key={message.key} value={message.value}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kafka_server = [\"192.168.1.22\"]\n",
    "topic = \"test_topic\"\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers = \":9092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kafka.consumer.group.KafkaConsumer object at 0x000001EC3381A2E0>\n"
     ]
    }
   ],
   "source": [
    "print(consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(consumer.subscribe(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Need at least one: key or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m producer \u001b[39m=\u001b[39m KafkaProducer(bootstrap_servers\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlocalhost:9092\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     producer\u001b[39m.\u001b[39;49msend(\n\u001b[0;32m      7\u001b[0m         topic\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpageview\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\producer\\kafka.py:573\u001b[0m, in \u001b[0;36mKafkaProducer.send\u001b[1;34m(self, topic, value, key, headers, partition, timestamp_ms)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Publish a message to a topic.\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \n\u001b[0;32m    541\u001b[0m \u001b[39mArguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39m        to obtain memory buffer prior to configured max_block_ms\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[39massert\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m1\u001b[39m), (\n\u001b[0;32m    572\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mNull messages require kafka >= 0.8.1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m), \u001b[39m'\u001b[39m\u001b[39mNeed at least one: key or value\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    574\u001b[0m key_bytes \u001b[39m=\u001b[39m value_bytes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: Need at least one: key or value"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "while True:\n",
    "    producer.send(\n",
    "        topic='pageview',\n",
    "        key=f\"{random.randrange(999)}\".encode(),\n",
    "        value=f\"{\\\"URL\\\":\\\"URL{random.randrange(999)\\\"}\".encode()\n",
    "    )\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m topic \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmy_topic\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Creating Kafka producer\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m producer \u001b[39m=\u001b[39m KafkaProducer(bootstrap_servers\u001b[39m=\u001b[39;49mbootstrap_servers)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Creating Kafka consumer\u001b[39;00m\n\u001b[0;32m     24\u001b[0m consumer \u001b[39m=\u001b[39m KafkaConsumer(bootstrap_servers\u001b[39m=\u001b[39mbootstrap_servers, auto_offset_reset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mearliest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\producer\\kafka.py:381\u001b[0m, in \u001b[0;36mKafkaProducer.__init__\u001b[1;34m(self, **configs)\u001b[0m\n\u001b[0;32m    378\u001b[0m reporters \u001b[39m=\u001b[39m [reporter() \u001b[39mfor\u001b[39;00m reporter \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mmetric_reporters\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metrics \u001b[39m=\u001b[39m Metrics(metric_config, reporters)\n\u001b[1;32m--> 381\u001b[0m client \u001b[39m=\u001b[39m KafkaClient(metrics\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metrics, metric_group_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mproducer\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    382\u001b[0m                      wakeup_timeout_ms\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mmax_block_ms\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    383\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig)\n\u001b[0;32m    385\u001b[0m \u001b[39m# Get auto-discovered version from client if necessary\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\client_async.py:244\u001b[0m, in \u001b[0;36mKafkaClient.__init__\u001b[1;34m(self, **configs)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     check_timeout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version_auto_timeout_ms\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_version(timeout\u001b[39m=\u001b[39;49mcheck_timeout)\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\myenv\\lib\\site-packages\\kafka\\client_async.py:927\u001b[0m, in \u001b[0;36mKafkaClient.check_version\u001b[1;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[39m# Timeout\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    926\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m--> 927\u001b[0m     \u001b[39mraise\u001b[39;00m Errors\u001b[39m.\u001b[39mNoBrokersAvailable()\n",
      "\u001b[1;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "# Kafka producer example\n",
    "def send_message(producer, topic, message):\n",
    "    try:\n",
    "        producer.send(topic, message.encode('utf-8')).get(timeout=10)\n",
    "        print(\"Message sent successfully\")\n",
    "    except KafkaError as e:\n",
    "        print(\"Failed to send message:\", str(e))\n",
    "\n",
    "# Kafka consumer example\n",
    "def consume_messages(consumer, topic):\n",
    "    consumer.subscribe(topic)\n",
    "\n",
    "    for message in consumer:\n",
    "        print(\"Received message:\", message.value.decode('utf-8'))\n",
    "\n",
    "# Kafka configuration\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "topic = 'my_topic'\n",
    "\n",
    "# Creating Kafka producer\n",
    "producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "# Creating Kafka consumer\n",
    "consumer = KafkaConsumer(bootstrap_servers=bootstrap_servers, auto_offset_reset='earliest')\n",
    "\n",
    "# Sending a message using the producer\n",
    "send_message(producer, topic, \"Hello, Kafka!\")\n",
    "\n",
    "# Consuming messages using the consumer\n",
    "consume_messages(consumer, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Closing Kafka producer and consumer\n",
    "producer.close()\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Kafka brokers available. Check your configuration and network connectivity.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError, NoBrokersAvailable\n",
    "\n",
    "# Kafka producer example\n",
    "def send_message(producer, topic, message):\n",
    "    try:\n",
    "        producer.send(topic, message.encode('utf-8')).get(timeout=10)\n",
    "        print(\"Message sent successfully\")\n",
    "    except KafkaError as e:\n",
    "        print(\"Failed to send message:\", str(e))\n",
    "\n",
    "# Kafka consumer example\n",
    "def consume_messages(consumer, topic):\n",
    "    consumer.subscribe(topic)\n",
    "\n",
    "    for message in consumer:\n",
    "        print(\"Received message:\", message.value.decode('utf-8'))\n",
    "\n",
    "# Kafka configuration\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "topic = 'my_topic'\n",
    "\n",
    "try:\n",
    "    # Creating Kafka producer\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "    # Creating Kafka consumer\n",
    "    consumer = KafkaConsumer(bootstrap_servers=bootstrap_servers, auto_offset_reset='earliest')\n",
    "\n",
    "    # Sending a message using the producer\n",
    "    send_message(producer, topic, \"Hello, Kafka!\")\n",
    "\n",
    "    # Consuming messages using the consumer\n",
    "    consume_messages(consumer, topic)\n",
    "\n",
    "except NoBrokersAvailable:\n",
    "    print(\"No Kafka brokers available. Check your configuration and network connectivity.\")\n",
    "\n",
    "finally:\n",
    "    # Closing Kafka producer and consumer\n",
    "    if 'producer' in locals():\n",
    "        producer.close()\n",
    "    if 'consumer' in locals():\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
